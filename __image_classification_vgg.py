# -*- coding: utf-8 -*-
"""image_classification_vgg.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1s5MPLefOf6FovWEl0TaNZuBssym9eNbC

VGG-16 with pre-trained weights
"""

from keras.layers import Dense, Flatten
from keras.models import Model
from keras.applications.vgg16 import VGG16
from keras.applications.vgg16 import preprocess_input
from keras.preprocessing.image import ImageDataGenerator
from keras.preprocessing.image import ImageDataGenerator
from keras.applications.vgg16 import preprocess_input
import numpy as np
import tensorflow as tf
from tensorflow import keras
from keras.layers import Dense, Flatten
from keras.models import Model
from keras.preprocessing.image import ImageDataGenerator
import pickle
from keras.callbacks import ModelCheckpoint, EarlyStopping
from PIL import Image


import ssl
ssl._create_default_https_context = ssl._create_unverified_context


IMAGE_SIZE = [64, 64]
batch_size = 16

vgg = VGG16(input_shape=IMAGE_SIZE + [3],
            weights='imagenet', include_top=False)
vgg.summary()

for layer in vgg.layers:
    layer.trainable = False

x = Flatten()(vgg.output)
prediction = Dense(6, activation='softmax')(x)
model = Model(inputs=vgg.input, outputs=prediction)

model.compile(
    loss='categorical_crossentropy',
    optimizer='adam',
    metrics=['accuracy']
)
model.summary()

train_path = 'seg_train/seg_train'
valid_path = 'seg_test/seg_test'

train_datagen = ImageDataGenerator(rescale=1./255,
                                   shear_range=0.2,
                                   zoom_range=0.2,
                                   horizontal_flip=True,
                                   preprocessing_function=preprocess_input)

test_datagen = ImageDataGenerator(rescale=1./255,
                                  preprocessing_function=preprocess_input)

training_set = train_datagen.flow_from_directory(train_path,
                                                 target_size=IMAGE_SIZE,
                                                 batch_size=batch_size,
                                                 class_mode='categorical')

test_set = test_datagen.flow_from_directory(valid_path,
                                            target_size=IMAGE_SIZE,
                                            batch_size=batch_size,
                                            class_mode='categorical')

print(training_set.class_indices)

epochs = 50
callbacks = [keras.callbacks.ModelCheckpoint("save_at_{epoch}.keras",
                                             verbose=1, save_best_only=True, save_freq="epoch", monitor="accuracy", mode="max"), EarlyStopping(patience=10)]


history = model.fit(
    training_set,
    validation_data=test_set,
    epochs=epochs,
    callbacks=callbacks,
    steps_per_epoch=len(training_set),
    validation_steps=len(test_set),
    batch_size=batch_size,
)

pred = model.evaluate(test_set)
print("Loss = " + str(pred[0]))
print("Test Accuracy = " + str(pred[1]*100))
print('Training Accuracy = ' + str(max(history.history['accuracy'])*100) + '%')

# save the model to pickle file
with open('vgg16.pickle', 'wb') as f:
    pickle.dump(model, f)

model.save('vgg16.h5')

img = tf.keras.preprocessing.image.load_img(
    "20057.jpg", target_size=IMAGE_SIZE
)
img_array = tf.keras.preprocessing.image.img_to_array(img)
img_array = tf.expand_dims(img_array, 0)
predictions = model.predict(img_array)
label_dict = {0: "building", 1: "forest",
              2: "glacier", 3: "mountain", 4: "sea", 5: "street"}
for i in range(len(predictions[0])):
    if predictions[0][i] == max(predictions[0]):
        print(label_dict[i])
